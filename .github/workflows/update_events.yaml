name: Update Gateway Events

# Trigger the workflow on a schedule and allow manual triggers
on:
  schedule:
    # Runs at 10:00 UTC daily (which is 3:00 AM MST / 4:00 AM MDT)
    - cron: '0 10 * * *'
  workflow_dispatch: # Allows you to click a "Run Now" button in GitHub for testing

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    # Give the action permission to write back to the repository
    permissions:
      contents: write

    steps:
      # Step 1: Check out your code so the script can access files
      - name: Checkout repository
        uses: actions/checkout@v3

      # Step 2: Set up a Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # Step 3: Install the libraries the scraper needs
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      # Step 4: Run the scraper script
      # This script will generate/overwrite 'gateway_events.json'
      - name: Run Scraper
        run: python scrape_events.py

      # Step 5: Check for changes and commit
      # This only runs if 'gateway_events.json' actually changed
      - name: Commit and push changes
        run: |
          git config --global user.name 'GitHub Action'
          git config --global user.email 'action@github.com'
          
          # Check if there are any changes to the json file
          if [[ -n $(git status -s gateway_events.json) ]]; then
            echo "Changes detected. Committing..."
            git add gateway_events.json
            git commit -m "Auto-update: Daily Gateway events refresh"
            git push
          else
            echo "No changes detected. Skipping commit."
          fi